{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P3_Twitter_sentiment_Analysis_Malay_Language_FeatureExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kKtfxy8o0gAUOAL3lVvd1gCij5HT5ve_",
      "authorship_tag": "ABX9TyO47bjlnyF4U6ZDiSv0Jpj3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hungryphobic/NLP-projects/blob/main/P3_Twitter_sentiment_Analysis_Malay_Language_FeatureExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDzdJuvbCVTv"
      },
      "source": [
        "# **Feature Extraction**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELQNr-OTCgFk"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "This phase is we take the cleaned dataset and performed feature extraction so that it can be use for classificaion task later on. Feature extraction which also known as vectorization; vectorization the general process of turning a collection of text documents into numerical feature vectors. Because it prepare the data in such it can be use; as sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
        "\n",
        "After some digging, there is some common vectorization method, technique wehich is bag if Word(BOW),count vectorizer and tfidf vectorizer. in this project I will compare the performance of count vectorizer and tfidf vectorizer; and mainly focus on tfidf as from my reading, **The hypothesis** is that tfidf vectorizer is better in term of performance and classification task later on. So,let's find out!\n",
        "\n",
        "\n",
        "For this part of project i will use scikit library. As scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
        "\n",
        "> * **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
        "* **counting** the occurrences of tokens in each document.\n",
        "* **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB1MvXesMB0t"
      },
      "source": [
        "# Import library package and module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwCpOA8haFv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b988e2d-de34-474b-c6fe-5233084c230d"
      },
      "source": [
        "#import module and package\n",
        "%%time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10 µs, sys: 2 µs, total: 12 µs\n",
            "Wall time: 14.8 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUT6YwvcNkvN"
      },
      "source": [
        "# just to try the vectorizer\n",
        "texts = [\"good movie\",\"not a good movie\",\"did not like\",\"i like it\",\"good one\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WARbd1KTUoAm"
      },
      "source": [
        "## **Pre-V**\n",
        "\n",
        "Just do simple vectorization for simple text data just to do an overview of both vectorizer\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBhPCUM6N6sW"
      },
      "source": [
        "### **Count vectorizer**\n",
        "---\n",
        "> CountVectorizer implements both tokenization and occurrence counting in a single class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qooIBMNPWS1"
      },
      "source": [
        "#### Initilize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJfjewSN5_w"
      },
      "source": [
        "cV = CountVectorizer(ngram_range=(1, 2),min_df=2) "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcRmWBIJPl2O"
      },
      "source": [
        "#### vectorize our corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elJKP9BwN07s"
      },
      "source": [
        "x = cV.fit_transform(texts)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC1xqrHIPvY-"
      },
      "source": [
        "#### View or vectorized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhoiC5gsOdaR",
        "outputId": "a2eeec53-c3d2-453b-c621-f94014fedee2"
      },
      "source": [
        "print(x.toarray())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 0 1 0]\n",
            " [1 1 0 1 1]\n",
            " [0 0 1 0 1]\n",
            " [0 0 1 0 0]\n",
            " [1 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "qSSt02LIOecQ",
        "outputId": "64df20fc-6641-468f-cf31-a2fe53a1eff8"
      },
      "source": [
        "pd.DataFrame(x.todense(), columns=cV.get_feature_names())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>good</th>\n",
              "      <th>good movie</th>\n",
              "      <th>like</th>\n",
              "      <th>movie</th>\n",
              "      <th>not</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   good  good movie  like  movie  not\n",
              "0     1           1     0      1    0\n",
              "1     1           1     0      1    1\n",
              "2     0           0     1      0    1\n",
              "3     0           0     1      0    0\n",
              "4     1           0     0      0    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfZYQTwEP0TI"
      },
      "source": [
        "### Tf idf Vectorizer\n",
        "---\n",
        "> **Tf** means ***term-frequency*** while **tf–idf**   means term-frequency times ***inverse document-frequency***: \n",
        "\n",
        "> `tf-idf(t,d) = tf(t,d) x idf(t)`\n",
        "\n",
        "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
        "\n",
        ">In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the **tf–idf transform**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcWDphqdP370"
      },
      "source": [
        "#### Initialize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8eZ7LybRpgJ"
      },
      "source": [
        "tV = TfidfVectorizer(min_df=2, ngram_range=(1,2))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1d56Fi2P4dI"
      },
      "source": [
        "#### Vectorize our Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGyC2io5Rp7v"
      },
      "source": [
        "y = tV.fit_transform(texts)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC5edDykP4md"
      },
      "source": [
        "#### View our vectorized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXE8cTJIRqqn",
        "outputId": "ab505643-5988-435a-c32d-4ba6ffaa81f9"
      },
      "source": [
        "print(y.toarray())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.50620441 0.60981846 0.         0.60981846 0.        ]\n",
            " [0.43218341 0.52064623 0.         0.52064623 0.52064623]\n",
            " [0.         0.         0.70710678 0.         0.70710678]\n",
            " [0.         0.         1.         0.         0.        ]\n",
            " [1.         0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ccQLOvShPEly",
        "outputId": "e8c3ec5f-baa4-4f70-cda7-915927498f77"
      },
      "source": [
        "pd.DataFrame(y.todense(), columns=tV.get_feature_names())"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>good</th>\n",
              "      <th>good movie</th>\n",
              "      <th>like</th>\n",
              "      <th>movie</th>\n",
              "      <th>not</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.506204</td>\n",
              "      <td>0.609818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.609818</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.432183</td>\n",
              "      <td>0.520646</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.520646</td>\n",
              "      <td>0.520646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.707107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       good  good movie      like     movie       not\n",
              "0  0.506204    0.609818  0.000000  0.609818  0.000000\n",
              "1  0.432183    0.520646  0.000000  0.520646  0.520646\n",
              "2  0.000000    0.000000  0.707107  0.000000  0.707107\n",
              "3  0.000000    0.000000  1.000000  0.000000  0.000000\n",
              "4  1.000000    0.000000  0.000000  0.000000  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orijj3KZVYiz"
      },
      "source": [
        "## V\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lc-IIuwcup6"
      },
      "source": [
        "### V-1-tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR98kcaGSipH"
      },
      "source": [
        "#load our data\n",
        "data1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/1.csv')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSSQ3HqIbcsM",
        "outputId": "f3b9ab66-a684-4f0f-b1d8-ec00d83d0bcb"
      },
      "source": [
        "# first have to remove nan value\n",
        "data1 = data1.dropna()\n",
        "data1.info()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 83 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Raw       83 non-null     object\n",
            " 1   CD-RE     83 non-null     object\n",
            " 2   Polarity  83 non-null     int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xMrWoeNbcx7"
      },
      "source": [
        "data1.to_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/1.csv')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Waw7zrXtaV2m"
      },
      "source": [
        "#load our non empty data\n",
        "data1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/1.csv')\n",
        "data1Text= data1['CD-RE']\n",
        "data1Text= list(data1Text)\n",
        "data1Polarity = data1['Polarity']"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6lQQsaffG8D"
      },
      "source": [
        "#### Vectorize our Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpuWV0gjavE6"
      },
      "source": [
        "data1Vector = tV.fit_transform(data1Text)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1awta7Fe8SJ"
      },
      "source": [
        "#### View our vectorized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsZmWXkUa94h",
        "outputId": "18c5e61a-7027-4aa1-edcc-e012244d2c84"
      },
      "source": [
        "print(data1Vector.toarray())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.31076123 0.         ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpMVlV-qc4pC"
      },
      "source": [
        "data1Vec = pd.DataFrame(data1Vector.todense(), columns=tV.get_feature_names())"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdFNX4kUdFZC"
      },
      "source": [
        "# merge our vectorized text data wuth theits labelled data\n",
        "data1ToProcess = pd.concat([data1Vec, data1Polarity], axis=1, ignore_index=False)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "yynQm5LId0XC",
        "outputId": "2e84feac-2bcc-4f04-fb79-2d8ddb74fb92"
      },
      "source": [
        "#view our merge vectorized data\n",
        "data1ToProcess.head(3)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>2021</th>\n",
              "      <th>24</th>\n",
              "      <th>500</th>\n",
              "      <th>50k</th>\n",
              "      <th>ada</th>\n",
              "      <th>allah</th>\n",
              "      <th>amp</th>\n",
              "      <th>awam</th>\n",
              "      <th>b40</th>\n",
              "      <th>balik</th>\n",
              "      <th>balik pinjaman</th>\n",
              "      <th>bantuan</th>\n",
              "      <th>bantuan khas</th>\n",
              "      <th>bawah</th>\n",
              "      <th>bayaran</th>\n",
              "      <th>bayaran balik</th>\n",
              "      <th>berada</th>\n",
              "      <th>beri</th>\n",
              "      <th>berjaya</th>\n",
              "      <th>berlaku</th>\n",
              "      <th>bertindak</th>\n",
              "      <th>bertujuan</th>\n",
              "      <th>betul</th>\n",
              "      <th>bulan</th>\n",
              "      <th>bulan ramadhan</th>\n",
              "      <th>covid</th>\n",
              "      <th>cukai</th>\n",
              "      <th>doa2</th>\n",
              "      <th>duit</th>\n",
              "      <th>ekonomi</th>\n",
              "      <th>fazura</th>\n",
              "      <th>gagal</th>\n",
              "      <th>geran</th>\n",
              "      <th>hari</th>\n",
              "      <th>hati</th>\n",
              "      <th>india</th>\n",
              "      <th>inisiatif</th>\n",
              "      <th>isu</th>\n",
              "      <th>jadi</th>\n",
              "      <th>...</th>\n",
              "      <th>pakej</th>\n",
              "      <th>pakej bantuan</th>\n",
              "      <th>peduli</th>\n",
              "      <th>pelan</th>\n",
              "      <th>pembelian</th>\n",
              "      <th>penyamun</th>\n",
              "      <th>penyamun perasuah</th>\n",
              "      <th>perasuah</th>\n",
              "      <th>perbelanjaan</th>\n",
              "      <th>perlu</th>\n",
              "      <th>permai</th>\n",
              "      <th>permai 2021</th>\n",
              "      <th>pinjaman</th>\n",
              "      <th>prihatin</th>\n",
              "      <th>program</th>\n",
              "      <th>program subsidi</th>\n",
              "      <th>rakyat</th>\n",
              "      <th>ramadhan</th>\n",
              "      <th>raya</th>\n",
              "      <th>rm2</th>\n",
              "      <th>rm200</th>\n",
              "      <th>salah</th>\n",
              "      <th>satu</th>\n",
              "      <th>subsidi</th>\n",
              "      <th>subsidi upah</th>\n",
              "      <th>tahu</th>\n",
              "      <th>tak</th>\n",
              "      <th>tambahan</th>\n",
              "      <th>tempoh</th>\n",
              "      <th>terima</th>\n",
              "      <th>terjejas</th>\n",
              "      <th>tidak</th>\n",
              "      <th>tiga</th>\n",
              "      <th>untuk</th>\n",
              "      <th>untuk pembelian</th>\n",
              "      <th>upah</th>\n",
              "      <th>warga</th>\n",
              "      <th>ya</th>\n",
              "      <th>zalim</th>\n",
              "      <th>Polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.256886</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.585824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.620033</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.432663</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.559288</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.559288</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.432663</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.68242</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   000  2021   24  500  50k  ...  upah  warga   ya  zalim  Polarity\n",
              "0  0.0   0.0  0.0  0.0  0.0  ...   0.0    0.0  0.0    0.0         1\n",
              "1  0.0   0.0  0.0  0.0  0.0  ...   0.0    0.0  0.0    0.0        -1\n",
              "2  0.0   0.0  0.0  0.0  0.0  ...   0.0    0.0  0.0    0.0        -1\n",
              "\n",
              "[3 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKyMKJIIfW-F"
      },
      "source": [
        "#### export our vectorized data for classificatioon task later on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jryfIhY1eDkk"
      },
      "source": [
        "data1ToProcess.to_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/1.csv')"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmvH8zZKxUX7"
      },
      "source": [
        "## AutomatedV()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TGZrpTQlyhT"
      },
      "source": [
        "def automatedV(min_file,max_file):\n",
        "  # step1: load cleaned data(contain nan value) --> data_i\n",
        "  # step 2: remove nan value datase\n",
        "  # step 3: saved the non empty cleaned data\n",
        "  # step 4: load again our non empty data and...\n",
        "  #   --> 1) data_i_Text \n",
        "  #   --> 2) data_i_Polarity\n",
        "  # step 5: vectorize our text data --> data_i_Vec\n",
        "  # step 6: merge our data <-- data_i_Vec concat data_i_Polarity \n",
        "  #   --> data_i_toProcess\n",
        "  # step 7: Export our dataset\n",
        "    # fp_1 is file path to import cleaned datset from cleaned dataset folder (refer step 1)\n",
        "    # fp_2 is file path to export nan removed dataset to nanRemove folder\n",
        "    #   also th epath where the datset will be import (refer step and step4)\n",
        "    # fp_3 is file path to saved our final dataset (refer step 7)\n",
        "  fp_1 = '/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/'\n",
        "  fp_2 = '/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/'\n",
        "  fp_3 = '/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/'\n",
        "  for i in range(min_file,max_file+1):\n",
        "    #initialized our variable\n",
        "    raw = 'data'+str(i)\n",
        "    fp1 = fp_1+str(i)+'.csv'\n",
        "    fp2 = fp_2+str(i)+'.csv'\n",
        "    fp3 = fp_3+str(i)+'.csv'\n",
        "    #step1\n",
        "    raw = pd.read_csv(fp1)\n",
        "    #step2\n",
        "    data = raw.dropna()\n",
        "    data.info()\n",
        "    #step3\n",
        "    data.to_csv(fp2)\n",
        "    #step4\n",
        "    data2 = pd.read_csv(fp2)\n",
        "    data2Text= data2['CD-RE']\n",
        "    data2Text= list(data2Text)\n",
        "    data2Polarity = data2['Polarity']\n",
        "    #step5\n",
        "    data2Vector = tV2.fit_transform(data2Text)\n",
        "    #step6\n",
        "    data2Vec = pd.DataFrame(data2Vector.todense(), columns=tV2.get_feature_names())\n",
        "      # merge our vectorized text data wuth their labelled data\n",
        "    data2ToProcess = pd.concat([data2Vec, data2Polarity], axis=1, ignore_index=False)\n",
        "    #step7\n",
        "    data2ToProcess.to_csv(fp3)\n",
        "    \n",
        "    print(fp1)\n",
        "    print(fp2)\n",
        "    print(fp3)\n"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCnV-01UlymO",
        "outputId": "4ffc4934-8ba2-4dfc-befe-d730bf240ba3"
      },
      "source": [
        "%%time\n",
        "automatedV(3,9)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 71 entries, 0 to 76\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Raw       71 non-null     object \n",
            " 1   CD-RE     71 non-null     object \n",
            " 2   Polarity  71 non-null     float64\n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 2.2+ KB\n",
            "   402  402 kapal  402 tenggelam   53  ...  video  viral  warga  Polarity\n",
            "0  0.0        0.0            0.0  0.0  ...    0.0    0.0    0.0      -1.0\n",
            "1  0.0        0.0            0.0  0.0  ...    0.0    0.0    0.0       0.0\n",
            "2  0.0        0.0            0.0  0.0  ...    0.0    0.0    0.0       0.0\n",
            "\n",
            "[3 rows x 103 columns]\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/3.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/3.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/3.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Raw       100 non-null    object\n",
            " 1   CD-RE     100 non-null    object\n",
            " 2   Polarity  100 non-null    int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 3.1+ KB\n",
            "    11   14  14 karim   19  ...  vaksin  wacana  wacana semalam  Polarity\n",
            "0  0.0  0.0       0.0  0.0  ...     0.0     0.0             0.0        -1\n",
            "1  0.0  0.0       0.0  0.0  ...     0.0     0.0             0.0        -1\n",
            "2  0.0  0.0       0.0  0.0  ...     0.0     0.0             0.0         0\n",
            "\n",
            "[3 rows x 260 columns]\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/4.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/4.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/4.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 15 entries, 0 to 14\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Raw       15 non-null     object\n",
            " 1   CD-RE     15 non-null     object\n",
            " 2   Polarity  15 non-null     int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 480.0+ bytes\n",
            "    11  11 kematian   13  ...  positif baharu  positif baru  Polarity\n",
            "0  0.0          0.0  0.0  ...             0.0      0.000000         1\n",
            "1  0.0          0.0  0.0  ...             0.0      0.000000         0\n",
            "2  0.0          0.0  0.0  ...             0.0      0.257136         0\n",
            "\n",
            "[3 rows x 33 columns]\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/5.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/5.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/5.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 74 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Raw       74 non-null     object\n",
            " 1   CD-RE     74 non-null     object\n",
            " 2   Polarity  74 non-null     int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.3+ KB\n",
            "    12  12 mei        14   14 hari  ...  wanita budak  warga   ya  Polarity\n",
            "0  0.0     0.0  0.000000  0.000000  ...           0.0    0.0  0.0         1\n",
            "1  0.0     0.0  0.225113  0.225113  ...           0.0    0.0  0.0         0\n",
            "2  0.0     0.0  0.000000  0.000000  ...           0.0    0.0  0.0         0\n",
            "\n",
            "[3 rows x 165 columns]\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/6.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/6.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/6.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 57 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Raw       57 non-null     object\n",
            " 1   CD-RE     57 non-null     object\n",
            " 2   Polarity  57 non-null     int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.8+ KB\n",
            "   ada  agama  ahli  anak  ...  undang ipcmc  undang undang  untuk  Polarity\n",
            "0  0.0    0.0   0.0   0.0  ...           0.0            0.0    0.0        -1\n",
            "1  0.0    0.0   0.0   0.0  ...           0.0            0.0    0.0         0\n",
            "2  0.0    0.0   0.0   0.0  ...           0.0            0.0    0.0        -1\n",
            "\n",
            "[3 rows x 67 columns]\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/7.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/7.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/7.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Raw       100 non-null    object\n",
            " 1   CD-RE     100 non-null    object\n",
            " 2   Polarity  100 non-null    int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 3.1+ KB\n",
            "    1a   1b   1c   1d  2006  ...  with  with tears   ya  zaman  Polarity\n",
            "0  0.0  0.0  0.0  0.0   0.0  ...   0.0         0.0  0.0    0.0        -1\n",
            "1  0.0  0.0  0.0  0.0   0.0  ...   0.0         0.0  0.0    0.0        -1\n",
            "2  0.0  0.0  0.0  0.0   0.0  ...   0.0         0.0  0.0    0.0        -1\n",
            "\n",
            "[3 rows x 182 columns]\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/8.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/8.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/8.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 98 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Raw       98 non-null     object\n",
            " 1   CD-RE     98 non-null     object\n",
            " 2   Polarity  98 non-null     int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 3.1+ KB\n",
            "        000        19  19 kes  2021  ...  yaakob siaran  zon  zon merah  Polarity\n",
            "0  0.000000  0.000000     0.0   0.0  ...            0.0  0.0        0.0        -1\n",
            "1  0.000000  0.200306     0.0   0.0  ...            0.0  0.0        0.0        -1\n",
            "2  0.571063  0.000000     0.0   0.0  ...            0.0  0.0        0.0        -1\n",
            "\n",
            "[3 rows x 237 columns]\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/9.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/nanRemove/9.csv\n",
            "/content/drive/MyDrive/Colab Notebooks/tweets_data/for feature extraction/clean dataset/vectorized/9.csv\n",
            "CPU times: user 666 ms, sys: 12.8 ms, total: 679 ms\n",
            "Wall time: 3.68 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKGRdGe5lypS"
      },
      "source": [
        ""
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnIw9-JDlytS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxjZT05ilyuu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}