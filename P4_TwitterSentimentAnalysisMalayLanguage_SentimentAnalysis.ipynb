{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P4_TwitterSentimentAnalysisMalayLanguage_SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNku1dqr1TiYg8O7EVwOzgW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hungryphobic/NLP-projects/blob/main/P4_TwitterSentimentAnalysisMalayLanguage_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-gx18pYmQ1F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJD5SG1uxa0U"
      },
      "source": [
        "# **Sentiment Analysis**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6v3Om7tpJFn"
      },
      "source": [
        "# Introduction \n",
        "supervised classificatiion task\n",
        "the idea:\n",
        "### 1. Naive bayes\n",
        "The Naive Bayes classifier uses the Bayes' theorem of conditional probabilities. For each feature, it calculates the probability for a class depending on the value of the feature. The Naive Bayes classifier calculates the class probabilities for each feature independently, which is equivalent to a strong (= naive) assumption of conditional independence of the features. Naive Bayes is a conditional probability model and models the probability of a class $C_{k}$ as follows:\n",
        "\n",
        "> $ P(C_{k}|x) = \\frac{1}{z}\\ P(C_{k})\\ \\prod_{i=1}^{n}\\  P(x_{i}|C_{k}) $\n",
        "\n",
        "### 2. Logistic Regression\n",
        "Logistic regression models the probabilities for classification problems with two possible outcomes. It's an extension of the linear regression model for classification problems.The linear regression model can work well for regression, but fails for classification.\n",
        "A solution for classification is logistic regression. Instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. The logistic function is defined as:\n",
        "\n",
        "> $logistic(n) = \\frac{1}{1 + exp(-n)}\\$\n",
        "  \n",
        "### 3. Decision Tree (DT)\n",
        "Linear regression and logistic regression models fail in situations where the relationship between features and outcome is nonlinear or where features interact with each other. So, supposedly DT can address that.Tree based models split the data multiple times according to certain cutoff values in the features. Through splitting, different subsets of the dataset are created, with each instance belonging to one subset. The final subsets are called terminal or leaf nodes and the intermediate subsets are called internal nodes or split nodes. To predict the outcome in each leaf node, the average outcome of the training data in this node is used. Trees can be used for classification and regression.\n",
        "\n",
        "The following formula describes the relationship between the outcome y and features x.:\n",
        "\n",
        "> $\\hat{y} = \\hat{f}(x) = \\sum_{m=1}^{M} c_{m}I \\{x \\in R_{m} \\}$ \n",
        "\n",
        "> Each instance falls into exactly one leaf node (=subset  $R_{m})$ . $I \\{x\\in R_{m}\\}$ is the identity function that returns 1 if $x$ is in the subset $ R_{m}$ and 0 otherwise. If an instance falls into a leaf node $R_{t}$ the predicted outcome is $\\hat{y} = c_{l}$ where $c_{l}$  is the average of all training instances in leaf node $R_{t}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jggCqlJ3q-dV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}