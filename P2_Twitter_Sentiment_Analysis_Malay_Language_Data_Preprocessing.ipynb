{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_Twitter Sentiment Analysis-Malay Language_Data Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1m6LRnz4prN51vYCNPnw3WMzXOqFEvL13",
      "authorship_tag": "ABX9TyNOK6Ch3T1C4OizgGdpj1J8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a150730/NLP-projects/blob/main/P2_Twitter_Sentiment_Analysis_Malay_Language_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAmspCqWxS5d"
      },
      "source": [
        "# 1) Import Module and package\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Xela0swjsH"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "from csv import reader\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUvLsw4BxZk-"
      },
      "source": [
        "# 2) Load Collected Raw Data\n",
        "---\n",
        "\n",
        "> The data topic and information details can be refered on Part 1 -3.1 of this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF7zIRfAxgmm"
      },
      "source": [
        "# dfKG = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/1_KerajaaanGagal_April29.csv')\n",
        "#dfMSSP = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/2_MakeSchoolASaferPlace_April29.csv')\n",
        "# dfKN = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/3_KRINanggala402_April29.csv')\n",
        "# dfSH = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/4_sinarharian_April29.csv')\n",
        "# dfCM = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/5_COVIDMalaysia_April29.csv')\n",
        "#dfKJK = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/6_KitaJagaKita_April29.csv')\n",
        "# dfJG = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/7_JusticeforGanapathy_April29.csv')\n",
        "# dfUPSR = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/8_UPSR_April29.csv')\n",
        "# dfARN = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/tweets_data/9_AstroRadioNews_April29.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK4rZf7XyVsv"
      },
      "source": [
        "### Test sentence string\n",
        "> * This few sample test is use just to test our model before proceed with raw dataset, to detect failure in line of code and etc...\n",
        "* The sample test text was selected from extracted raw dataset. the sample est should include all the main concern for our preprocessing purpose. It should contains noisy text (Uppercase, mispell word, @mention tag, #hashtag, emoji/emoticon, stopwords, Non-Malay word and etc...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnOe6kJgykdm"
      },
      "source": [
        "TS1 = '@Marina_Ibrahims Kebabian PN makin terserlah dan muhyiddin kimak \\n#KerajaanGagal' #contain ,#tag, @mention, Uppercase extra character\n",
        "TS2 = '@kuasasiswa you made my day!👏😂\\nso .lma dh X rasa @ujbu57ubj_78BJ gmbira mcm ni 👍 \\n#Keraja...' #contain misspell word, emoticon/emoji, English word\n",
        "TS3 = \"@Izzud1n Omg 😭🤚🏼 1234 like, if we doesn't even get good results for upsr. not that we couldn't do well in sekolah menengah. aigoo https://t.co/K4GiA9l1Iw\" #contain link, emoticon/emoji, English word, "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mREMYhqdzLMa"
      },
      "source": [
        "# 3) Data PreProcessing\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEE-0lc507bj"
      },
      "source": [
        "## 3.1) Overall idea\n",
        "\n",
        "> For this project purpose I will be covered 2 procesing tools which is using re- Regular Expression (standard python module for handling both pattern and string) *and combination of other techniques*.This module provides regular expression matching operations similar to those found in Perl. more info check - https://docs.python.org/3/library/re.html.\n",
        "\n",
        "> Next is just PreProcesisng using **Malaya** nltk .which is a toolkit that known for processing Malay Language. **Malaya** is a Natural-Language-Toolkit library for bahasa Malaysia, powered by Deep Learning Tensorflow. More info check - https://malaya.readthedocs.io/en/latest/index.html#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UynhMqnF1XsZ"
      },
      "source": [
        "## 3.2) PreProcessing 1- reMixin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC9tHN592JiE"
      },
      "source": [
        "###The Strategy/Methodology\n",
        "---\n",
        "> reMixin preprocesing is my propose preprocessing step which the overall step is combination of RE module and other other technique; as follows:\n",
        "* Lowercasing the character --using python string manipulation\n",
        "* Removing :\n",
        "  * #Hashtag;i.e) #KerajaanGagal\n",
        "  * @Mention or/@user; i.e) @mohd25\n",
        "  * link; i.e) https://t.co/iCT2wJuiNLy6uiwENsx3\n",
        "* 2 option for emoji and emoticon either:\n",
        "  * 1) Remove \n",
        "  * 2) Translate them into text (Preferable to maintain the information)\n",
        "* Identify English word and Translate them to Malay word -- using Malaya transformer translation\n",
        "* Spelling Correction using Malaya probability spelling correction tools\n",
        "* Removing stopwards and removing shortform word ( because most of them is stopwords and does not really affect the for clasification) i.e; yg == yang; utk == untuk; Except following shortform:\n",
        "  * 'x', 'tk'  == 'tak' or tidak' whcih means 'no'\n",
        "  * 'takda' or 'xda' == ' tak ada' or 'tidak ada' means ' do not have'.\n",
        "* Removing the NonAlphanumeric and special character -re\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVoHhK_I5qmo"
      },
      "source": [
        "#### *stripNonAlphaNumeric(string)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPX9qHtZzFMI"
      },
      "source": [
        "def stripNonAlphaNum(text):\n",
        "  text = re.compile(r'\\W+', re.UNICODE).split(text)   \n",
        "  return ' '.join(text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGy6AOV9FQUd"
      },
      "source": [
        "#### *StripHashTag(String)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ep6M2AR_dme"
      },
      "source": [
        "def stripHashTag(text):\n",
        "  text = re.compile(r'#\\S+', re.UNICODE).split(text)   \n",
        "  return ' '.join(text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bq4DpdXFQxX"
      },
      "source": [
        "#### *StripMention(String)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYZf9upzAGoB"
      },
      "source": [
        "def stripMention(text):\n",
        "  text = re.compile(r'@\\S+', re.UNICODE).split(text)   \n",
        "  return ' '.join(text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIEUtBmfFRY0"
      },
      "source": [
        "#### *StripHyperLink(String)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09VJrlIXBWlL"
      },
      "source": [
        "def stripHyperLink(text):\n",
        "  text = re.compile(r'https?:\\/\\/\\S+', re.UNICODE).split(text)   \n",
        "  return ' '.join(text)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFXo8F_sh8WP"
      },
      "source": [
        "#### *stripUnderscore(String)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ied0MGOp-k3G"
      },
      "source": [
        "def stripUnderscore(text):\n",
        "  text = re.sub(r'_', ' ', text)\n",
        "  return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTcaeUfi6Mvr"
      },
      "source": [
        "#### *remove_emoji(string)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lIBEOFG6hp8"
      },
      "source": [
        "def removeEmoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNXrgMrt7CQS"
      },
      "source": [
        "#### *emojiToText(string)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FABS-9yI7Bxo"
      },
      "source": [
        "#COnvert/translate emoji to text\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/Emoji_Dict.p', 'rb') as fp:\n",
        "    Emoji_Dict = pickle.load(fp)\n",
        "    Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
        "\n",
        "def emojiToText(text):\n",
        "    for emoj in Emoji_Dict:\n",
        "      text = re.sub(r'('+emoj+')', Emoji_Dict[emoj], text)          \n",
        "    return text"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZssxSEVAt-W"
      },
      "source": [
        "#### *removeStopWord(string)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMb4bnsUnKtt"
      },
      "source": [
        "stopWord = open('/content/drive/MyDrive/Colab Notebooks/MalayStopWord.csv', 'r').read().split()\n",
        "def removeStopWord(text):\n",
        "  text = text.split()\n",
        "  text = [word for word in text if word not in stopWord]\n",
        "  return ' '.join(text)\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odfCJ1H_58Xz"
      },
      "source": [
        "#### ***reMixin(string)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MUU7IQ25Wby"
      },
      "source": [
        "def reMixin(text):\n",
        "  text = text.lower()\n",
        "  text = emojiToText(text) \n",
        "  text = stripHashTag(text)\n",
        "  text = stripMention(text)\n",
        "  text = stripHyperLink(text)\n",
        "  text = stripNonAlphaNum(text)\n",
        "  text = stripUnderscore(text)\n",
        "  text = removeStopWord(text)\n",
        "  return text"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpNxy-kJ7uPk",
        "outputId": "4988045e-5388-4282-ee0c-70876c390876"
      },
      "source": [
        "print(TS1)\n",
        "print(reMixin(TS1))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@Marina_Ibrahims Kebabian PN makin terserlah dan muhyiddin kimak \n",
            "#KerajaanGagal\n",
            "kebabian pn terserlah muhyiddin kimak\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDhcN4p-79_U",
        "outputId": "346f527d-6955-4eaa-ee67-1adacac49adf"
      },
      "source": [
        "print(TS2)\n",
        "print(reMixin(TS2))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@kuasasiswa you made my day!👏😂\n",
            "so .lma dh X rasa @ujbu57ubj_78BJ gmbira mcm ni 👍 \n",
            "#Keraja...\n",
            "you made my day clapping hands face with tears of joy so lma dh x rasa gmbira mcm ni thumbs up\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlg_IueLFyRA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}